{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ì¿¼ë¦¬ í™•ì¥ (Query Expansion)\n",
    "\n",
    "--- \n",
    "\n",
    "## 1. ê°œë… ì´í•´\n",
    "\n",
    "### 1.1 ì¿¼ë¦¬ í™•ì¥ì´ë€?\n",
    "\n",
    "- ì¿¼ë¦¬ í™•ì¥(Query Expansion)ì€ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ ë³´ë‹¤ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ ë³€í˜•í•˜ê±°ë‚˜ í™•ì¥í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. \n",
    "- RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ì˜ í’ˆì§ˆì€ ìµœì¢… ë‹µë³€ì˜ í’ˆì§ˆì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œì´ë¯€ë¡œ, ì ì ˆí•œ ì¿¼ë¦¬ í™•ì¥ ê¸°ë²•ì„ í†µí•´ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 1.2 ì¿¼ë¦¬ í™•ì¥ì˜ í•„ìš”ì„±\n",
    "\n",
    "#### ğŸ” **ê²€ìƒ‰ì˜ í•œê³„**\n",
    "- **ì–´íœ˜ ë¶ˆì¼ì¹˜ ë¬¸ì œ**: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê°„ ìš©ì–´ ì°¨ì´\n",
    "- **ëª¨í˜¸í•œ ì§ˆë¬¸**: ë¶ˆëª…í™•í•˜ê±°ë‚˜ ë§¥ë½ì´ ë¶€ì¡±í•œ ì¿¼ë¦¬\n",
    "- **ë³µì¡í•œ ì§ˆë¬¸**: ì—¬ëŸ¬ í•˜ìœ„ ì§ˆë¬¸ì„ í¬í•¨í•˜ëŠ” ë³µí•© ì§ˆë¬¸\n",
    "- **ì˜ë„ íŒŒì•… ì–´ë ¤ì›€**: ì‚¬ìš©ìì˜ ì‹¤ì œ ì˜ë„ì™€ í‘œë©´ì  ì§ˆë¬¸ì˜ ì°¨ì´\n",
    "\n",
    "#### ğŸ¯ **í•´ê²° ë°©í–¥**\n",
    "- **ì˜ë¯¸ë¡ ì  í™•ì¥**: ë™ì˜ì–´, ê´€ë ¨ ê°œë… ì¶”ê°€\n",
    "- **êµ¬ì¡°ì  ë¶„í•´**: ë³µì¡í•œ ì§ˆë¬¸ì„ ë‹¨ìˆœí•œ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´\n",
    "- **ë§¥ë½ì  í™•ì¥**: ë°°ê²½ ì§€ì‹ê³¼ ì¼ë°˜ì  ê°œë… ì¶”ê°€\n",
    "- **ê°€ìƒ ë¬¸ì„œ ìƒì„±**: ì´ìƒì  ë‹µë³€ì„ í†µí•œ ê²€ìƒ‰ ê°œì„ \n",
    "\n",
    "### 1.3 ì¿¼ë¦¬ í™•ì¥ ë°©ë²•ë¡  ë¶„ë¥˜\n",
    "\n",
    "| ë°©ë²•ë¡  | í•µì‹¬ ì•„ì´ë””ì–´ | ì¥ì  | ë‹¨ì  | ì ìš© ìƒí™© |\n",
    "|--------|---------------|------|------|-----------|\n",
    "| **Query Reformulation** | LLMìœ¼ë¡œ ì§ˆë¬¸ ì¬ì‘ì„± | êµ¬í˜„ ê°„ë‹¨, ì¦‰ì‹œ ì ìš© | ë‹¨ì¼ ë³€í˜•ë§Œ ìƒì„± | ì¼ë°˜ì ì¸ ì§ˆë¬¸ ê°œì„  |\n",
    "| **Multi Query** | ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆë¬¸ ìƒì„± | ê²€ìƒ‰ ë‹¤ì–‘ì„± ì¦ê°€ | ê³„ì‚° ë¹„ìš© ì¦ê°€ | ëª¨í˜¸í•œ ì§ˆë¬¸ ì²˜ë¦¬ |\n",
    "| **Decomposition** | ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´ | ì²´ê³„ì  ì ‘ê·¼ | ë¶„í•´ ì •í™•ë„ ì˜ì¡´ | ë³µí•©ì  ì§ˆë¬¸ |\n",
    "| **Step-Back Prompting** | ì¼ë°˜ì  ë§¥ë½ì—ì„œ êµ¬ì²´ì  ë‹µë³€ìœ¼ë¡œ | í¬ê´„ì  ì´í•´ | ì¶”ê°€ ê²€ìƒ‰ í•„ìš” | ì „ë¬¸ì /ë³µì¡í•œ ì§ˆë¬¸ |\n",
    "| **HyDE** | ê°€ìƒ ë‹µë³€ ë¬¸ì„œ ìƒì„± | ì˜ë¯¸ì  ì •ë ¬ ìš°ìˆ˜ | í™˜ê° ìœ„í—˜ | Zero-shot ìƒí™© |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env í™˜ê²½ë³€ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# LangChain í•µì‹¬\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, BaseOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# ê²€ìƒ‰ ë° ë²¡í„° ì €ì¥\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing ì„¤ì •`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing ì—¬ë¶€ë¥¼ í™•ì¸ (true: langsmith ì¶”ì  í™œì„±í™”, false: langsmith ì¶”ì  ë¹„í™œì„±í™”)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector_store(embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\"), collection_name=\"hybrid_search_db\", persist_directory = \"./local_chroma_db\"):\n",
    "    \"\"\"\n",
    "    ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±\n",
    "    \n",
    "    Returns:\n",
    "        Chroma: ë²¡í„° ì €ì¥ì†Œ ê°ì²´\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        # ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹œë„\n",
    "        vector_store = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "        )\n",
    "        \n",
    "        doc_count = vector_store._collection.count()\n",
    "        if doc_count > 0:\n",
    "            print(f\"âœ… ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ: {doc_count}ê°œ ë¬¸ì„œ\")\n",
    "            return vector_store\n",
    "        else:\n",
    "            print(\"âš ï¸ ë¹ˆ ë²¡í„° ì €ì¥ì†Œì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\")\n",
    "            return vector_store\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”\n",
    "chroma_db = initialize_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) ë°±í„° ê²€ìƒ‰ê¸° ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ retriever ì´ˆê¸°í™”\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **Query Reformulation** \n",
    "\n",
    "- **Query Reformulation**ì€ **LLM**ì„ í™œìš©í•´ ì›ë³¸ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì¬êµ¬ì„±\n",
    "- **ë™ì˜ì–´ í™•ì¥**ê³¼ **í‚¤ì›Œë“œ ì¶”ê°€**ë¥¼ í†µí•´ ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ë²”ìœ„ë¥¼ í™•ì¥\n",
    "- ëª¨í˜¸í•œ ì§ˆë¬¸ì„ **ëª…í™•í•˜ê²Œ êµ¬ì²´í™”**í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "- í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ë‹¤ì–‘í•œ ë³€í˜• ì¿¼ë¦¬**ë¥¼ ìƒì„±í•˜ì—¬ ê²€ìƒ‰ ì»¤ë²„ë¦¬ì§€ í™•ëŒ€\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_rewrite.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[ì¶œì²˜] https://arxiv.org/abs/2305.14283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ì¿¼ë¦¬ ë¦¬í¬ë®¬ë ˆì´ì…˜ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "reformulation_template = \"\"\"ë‹¤ìŒ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”:\n",
    "[ì§ˆë¬¸]\n",
    "{question}\n",
    "\n",
    "ë‹¤ìŒ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì„¸ìš”:\n",
    "1. ë™ì˜ì–´ ì¶”ê°€\n",
    "2. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ í¬í•¨\n",
    "3. ê´€ë ¨ëœ ê°œë… í™•ì¥\n",
    "\n",
    "[ì¬ì‘ì„±ëœ ì§ˆë¬¸]\n",
    "\"\"\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_template(reformulation_template)\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "\n",
    "# ì¿¼ë¦¬ ë¦¬í¬ë®¬ë ˆì´ì…˜ ì²´ì¸ ìƒì„±\n",
    "reformulation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ì²´ì¸ ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "reformulated_query = reformulation_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "pprint(f\"ë¦¬í¬ë®¬ë ˆì´ì…˜ëœ ì¿¼ë¦¬: \\n{reformulated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¦¬í¬ë®¬ë ˆì´ì…˜ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰\n",
    "retrieved_docs = chroma_k_retriever.invoke(reformulated_query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ê¸° ìƒì„± (LCEL)\n",
    "reformulation_retriever = reformulation_chain | chroma_k_retriever\n",
    "\n",
    "# ì¿¼ë¦¬ ë¦¬í¬ë®¬ë ˆì´ì…˜ ê²€ìƒ‰ê¸° ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "retrieved_docs = reformulation_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_dataset(file_path):\n",
    "    \"\"\"\n",
    "    í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): í‰ê°€ ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: í‰ê°€ ë°ì´í„°ì…‹\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹\")\n",
    "        \n",
    "        print(f\"âœ… í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ: {len(df)}ê°œ ì§ˆë¬¸\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "eval_df = load_evaluation_dataset(\"./data/synthetic_testset.csv\")\n",
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_evaluation_data(df):\n",
    "    \"\"\"\n",
    "    í‰ê°€ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸, ì •ë‹µ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸)\n",
    "    \"\"\"\n",
    "    questions = df['user_input'].tolist()\n",
    "    \n",
    "    # ì •ë‹µ ë¬¸ì„œ íŒŒì‹±\n",
    "    reference_contexts = []\n",
    "    for contexts in df['reference_contexts']:\n",
    "        if isinstance(contexts, str):\n",
    "            # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "            context_list = eval(contexts)\n",
    "        else:\n",
    "            context_list = contexts\n",
    "        \n",
    "        # Document ê°ì²´ë¡œ ë³€í™˜\n",
    "        docs = [Document(page_content=ctx) for ctx in context_list]\n",
    "        reference_contexts.append(docs)\n",
    "    \n",
    "    return questions, reference_contexts\n",
    "\n",
    "# í‰ê°€ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "questions, reference_contexts = prepare_evaluation_data(eval_df)\n",
    "\n",
    "# í‰ê°€ ë°ì´í„° í™•ì¸\n",
    "for i, (q, refs) in enumerate(zip(questions[:3], reference_contexts[:3])):\n",
    "    print(f\"\\n[ì§ˆë¬¸ {i+1}]\")\n",
    "    print(f\"ì§ˆë¬¸: {q}\")\n",
    "    print(f\"ì •ë‹µ ë¬¸ì„œ: {len(refs)}ê°œ\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] ë‚´ìš©: {ref.page_content[:50]}...\")  # ë‚´ìš© ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# í‰ê°€ ë°ì´í„° í™•ì¸\n",
    "for i, (q, refs) in enumerate(zip(questions[-3:], reference_contexts[-3:])):\n",
    "    print(f\"\\n[ì§ˆë¬¸ {i+1}]\")\n",
    "    print(f\"ì§ˆë¬¸: {q}\")\n",
    "    print(f\"ì •ë‹µ ë¬¸ì„œ: {len(refs)}ê°œ\")\n",
    "    for j, ref in enumerate(refs):\n",
    "        print(f\"  [{j+1}] ë‚´ìš©: {ref.page_content[:50]}...\")  # ë‚´ìš© ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranx-k ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•´ì„œ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€\n",
    "from ranx_k.evaluation import evaluate_with_ranx_similarity\n",
    "\n",
    "# ranx-k í‰ê°€ ì‹¤í–‰ (embedding ì ìˆ˜ê°€ ë†’ì€ ê²½ìš°) -> ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ í‰ê°€\n",
    "chroma_results = evaluate_with_ranx_similarity(\n",
    "    retriever=chroma_k_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformulation_results = evaluate_with_ranx_similarity(\n",
    "    retriever=reformulation_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **Multi Query** \n",
    "\n",
    "- **Multi Query**ëŠ” **Retrieverì˜ LLM**ì„ ì‚¬ìš©í•´ ë‹¨ì¼ ì§ˆë¬¸ì„ ë‹¤ìˆ˜ì˜ ì¿¼ë¦¬ë¡œ í™•ì¥\n",
    "- ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•´ **ë‹¤ì–‘í•œ ê´€ì **ê³¼ **í‘œí˜„ ë°©ì‹**ìœ¼ë¡œ ì¿¼ë¦¬ ìë™ ìƒì„±\n",
    "- **LLMì˜ ìƒì„± ëŠ¥ë ¥**ì„ í™œìš©í•´ ê²€ìƒ‰ ë²”ìœ„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥\n",
    "- ê²€ìƒ‰ì˜ **ë‹¤ì–‘ì„±**ê³¼ **í¬ê´„ì„±**ì´ í–¥ìƒë˜ì–´ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ í™•ë¥  ì¦ê°€\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/multi-query.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "[ì¶œì²˜] https://arxiv.org/abs/2411.13154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) MultiQueryRetriever í™œìš©`\n",
    "\n",
    "- https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©€í‹° ì¿¼ë¦¬ ìƒì„±\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™” (ë©€í‹° ì¿¼ë¦¬ ìƒì„±ìš©)\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-mini',\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# ê¸°ë³¸ retrieverë¥¼ ì´ìš©í•œ ë©€í‹° ì¿¼ë¦¬ ìƒì„± \n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_k_retriever, llm=llm\n",
    ")\n",
    "\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "retrieved_docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) Custom Prompt í™œìš©`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# ì¶œë ¥ íŒŒì„œ: LLM ê²°ê³¼ë¥¼ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Split the text into lines and remove empty lines.\"\"\"\n",
    "        return [line.strip() for line in text.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "\n",
    "# ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Generate three different versions of the given user question to retrieve relevant documents from a vector database. The goal is to reframe the question from various perspectives to overcome limitations of distance-based similarity search.\n",
    "\n",
    "    The generated questions should have the following characteristics:\n",
    "    1. Maintain the core intent of the original question but use different expressions or viewpoints.\n",
    "    2. Include synonyms or related concepts where possible.\n",
    "    3. Slightly broaden or narrow the scope of the question to potentially include diverse relevant information.\n",
    "\n",
    "    Write each question on a new line and include only the questions.\n",
    "\n",
    "    [Original question]\n",
    "    {question}\n",
    "    \n",
    "    [Alternative questions]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# ë©€í‹°ì¿¼ë¦¬ ì²´ì¸ êµ¬ì„±\n",
    "multiquery_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "result = multiquery_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"ìƒì„±ëœ ëŒ€ì•ˆ ì§ˆë¬¸ë“¤:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸° ìƒì„±\n",
    "multi_query_custom_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever, # ê¸°ë³¸ retriever\n",
    "    llm_chain=multiquery_chain,   # ë©€í‹°ì¿¼ë¦¬ ì²´ì¸\n",
    "    parser_key=\"lines\"            # \"lines\": ì¶œë ¥ íŒŒì„œì˜ í‚¤\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_custom_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_results = evaluate_with_ranx_similarity(\n",
    "    retriever=multi_query_custom_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **Decomposition** \n",
    "\n",
    "- **ë‹¨ê³„ë³„ ë¶„í•´ ì „ëµ**ì„ í†µí•´ ë³µì¡í•œ ì§ˆë¬¸ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•¨\n",
    "- ê° í•˜ìœ„ ì§ˆë¬¸ë§ˆë‹¤ **ë…ë¦½ì ì¸ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤**ë¥¼ ì§„í–‰í•˜ì—¬ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚´\n",
    "- **LEAST-TO-MOST PROMPTING**ì„ í™œìš©í•˜ì—¬ ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²° ë°©ì‹ì„ êµ¬í˜„í•¨\n",
    "- ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©ë²•ë¡ \n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_decomposition.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[ì¶œì²˜] https://arxiv.org/pdf/2205.10625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to decompose the given input question into multiple sub-questions. \n",
    "    The goal is to break down the input into a set of sub-problems/sub-questions that can be answered independently.\n",
    "\n",
    "    Follow these guidelines to generate the sub-questions:\n",
    "    1. Cover various aspects related to the core topic of the original question.\n",
    "    2. Each sub-question should be specific, clear, and answerable independently.\n",
    "    3. Ensure that the sub-questions collectively address all important aspects of the original question.\n",
    "    4. Consider temporal aspects (past, present, future) where applicable.\n",
    "    5. Formulate the questions in a direct and concise manner.\n",
    "\n",
    "    [Input question] \n",
    "    {question}\n",
    "\n",
    "    [Sub-questions (5)]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# ì¿¼ë¦¬ ìƒì„± ì²´ì¸\n",
    "decomposition_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "result = decomposition_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"ìƒì„±ëœ ì„œë¸Œ ì§ˆë¬¸ë“¤:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ê¸° ìƒì„±\n",
    "multi_query_decompostion_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever,    # ê¸°ë³¸ retriever\n",
    "    llm_chain=decomposition_chain,   # ì„œë¸Œ ì§ˆë¬¸ ìƒì„± ì²´ì¸\n",
    "    parser_key=\"lines\"               # \"lines\": ì¶œë ¥ íŒŒì„œì˜ í‚¤\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_decompostion_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_results = evaluate_with_ranx_similarity(\n",
    "    retriever=multi_query_decompostion_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) **Step-Back Prompting**\n",
    "\n",
    "- **ë‹¨ê³„ì  í›„í‡´ ë°©ì‹**ì„ í†µí•´ êµ¬ì²´ì  ì§ˆë¬¸ì„ ì¼ë°˜ì  ë§¥ë½ì—ì„œ ì ‘ê·¼í•¨\n",
    "- **ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰**ìœ¼ë¡œ ë„“ì€ ê´€ì ì—ì„œ êµ¬ì²´ì  ë‹µë³€ìœ¼ë¡œ ì¢í˜€ë‚˜ê°\n",
    "- **í¬ê´„ì  ì ‘ê·¼ë²•**ì„ í™œìš©í•˜ì—¬ ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ë†’ì„\n",
    "- ì¼ë°˜ì  ë§¥ë½ì—ì„œ ì‹œì‘í•˜ì—¬ êµ¬ì²´ì  í•´ë‹µì„ ì°¾ì•„ê°€ëŠ” ì²´ê³„ì  ì ‘ê·¼ ë°©ì‹\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_stepback.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[ì¶œì²˜] https://arxiv.org/pdf/2310.06117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Step-Back ì§ˆë¬¸ ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Few Shot ì˜ˆì œ - (êµ¬ì²´ì  ì§ˆë¬¸, í¬ê´„ì  ì§ˆë¬¸) ìŒ\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ì• í”Œì˜ M1 ì¹© ê°œë°œì´ ê¸°ì—… ê°€ì¹˜ì— ë¯¸ì¹œ ì˜í–¥ì€?\",\n",
    "        \"output\": \"ê¸°ì—…ì˜ í•µì‹¬ ê¸°ìˆ  ë‚´ì¬í™”ê°€ ê²½ìŸìš°ìœ„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¬´ì—‡ì¸ê°€?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì•„ë§ˆì¡´ì˜ AWSê°€ ìˆ˜ìµì„±ì— ê¸°ì—¬í•˜ëŠ” ë°©ì‹ì€?\",\n",
    "        \"output\": \"ê¸°ì—…ì˜ ìƒˆë¡œìš´ ì‚¬ì—… ì˜ì—­ í™•ì¥ì´ ìˆ˜ìµ êµ¬ì¡°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¬´ì—‡ì¸ê°€?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"í† ìš”íƒ€ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê¸°ìˆ  ì „ëµì˜ í•µì‹¬ì€?\",\n",
    "        \"output\": \"ìë™ì°¨ ì‚°ì—…ì—ì„œ ì¹œí™˜ê²½ ê¸°ìˆ  í˜ì‹ ì´ ê¸°ì—… ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¬´ì—‡ì¸ê°€?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Step-Back ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸\n",
    "step_back_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"ë‹¹ì‹ ì€ ê¸°ì—… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹ì • ê¸°ì—…ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë‹¹ ì‚°ì—…ì´ë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ë°˜ì˜ ì¼ë°˜ì ì¸ ê´€ì ì—ì„œ \n",
    "                ì¬í•´ì„í•˜ëŠ” ê²ƒì´ ì„ë¬´ì…ë‹ˆë‹¤. ì‚°ì—… ë™í–¥, ê²½ìŸ êµ¬ë„, ê¸°ìˆ  í˜ì‹ , ì‚¬ì—… ëª¨ë¸ ë“±ì˜ ê´€ì ì—ì„œ ë” í¬ê´„ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ \n",
    "                ë°”ê¾¸ì–´ ì£¼ì„¸ìš”. ë‹¤ìŒì€ ì˜ˆì‹œì…ë‹ˆë‹¤:\"\"\"\n",
    "            ),\n",
    "            few_shot_prompt,\n",
    "            (\"user\", \"{question}\"),\n",
    "        ])\n",
    "\n",
    "# Step-Back ì²´ì¸ ìƒì„±\n",
    "step_back_chain = step_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step-Back ì§ˆë¬¸ ìƒì„±\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "step_back_question = step_back_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "print(f\"Step-Back ì§ˆë¬¸: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-Back ê²€ìƒ‰ê¸° ìƒì„±\n",
    "step_back_retriever = step_back_chain | chroma_k_retriever\n",
    "\n",
    "# Step-Back ê²€ìƒ‰ ì‹¤í–‰\n",
    "retrieved_docs = step_back_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepback_results = evaluate_with_ranx_similarity(\n",
    "    retriever=step_back_retriever,\n",
    "    questions=questions, \n",
    "    reference_contexts=reference_contexts,\n",
    "    k=5,\n",
    "    method='kiwi_rouge',  \n",
    "    similarity_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ìµœì¢… ë‹µë³€ ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”\n",
    "response_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"ë‹¹ì‹ ì€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "            ì¼ë°˜ ì»¨í…ìŠ¤íŠ¸:\n",
    "            {normal_context}\n",
    "            \n",
    "            ê¸°ë³¸ ê°œë… ì»¨í…ìŠ¤íŠ¸:\n",
    "            {step_back_context}\n",
    "            \n",
    "            ì›ë˜ ì§ˆë¬¸: {question}\n",
    "            \n",
    "            ë‹µë³€:\"\"\"\n",
    "        )\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ì²´ì¸\n",
    "answer_chain = (\n",
    "            {\n",
    "                \"normal_context\": chroma_k_retriever,\n",
    "                \"step_back_context\": step_back_retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | response_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "# ë‹µë³€ ìƒì„±\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "answer = answer_chain.invoke(query)\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "print(f\"ë‹µë³€: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) **HyDE** (Hypothetical Document Embedding)\n",
    "\n",
    "- **ê°€ìƒ ë¬¸ì„œ ìƒì„±**ì„ í†µí•´ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ê°€ìƒì˜ ì´ìƒì ì¸ ë‹µë³€ ë¬¸ì„œë¥¼ LLMìœ¼ë¡œ ìƒì„±í•¨\n",
    "- ìƒì„±ëœ ë¬¸ì„œì˜ **ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰**ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì„œì™€ ë§¤ì¹­ì„ ìˆ˜í–‰í•¨\n",
    "- **ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ ë°©ì‹**ìœ¼ë¡œ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•¨\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_HyDE.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[ì¶œì²˜] https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê°€ìƒ ë¬¸ì„œ ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# HyDEë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "template = \"\"\"ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì´ìƒì ì¸ ë¬¸ì„œ ë‚´ìš©ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\n",
    "ë¬¸ì„œëŠ” í•™ìˆ ì ì´ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë¬¸ì„œ ë‚´ìš©:\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM ëª¨ë¸ ì´ˆê¸°í™”\n",
    "hyde_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# ë¬¸ì„œ ìƒì„± ì²´ì¸ ìƒì„±\n",
    "hyde_chain = hyde_prompt | hyde_llm | StrOutputParser()\n",
    "\n",
    "# ë¬¸ì„œ ìƒì„± ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "hypothetical_doc = hyde_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "print(f\"ë¬¸ì„œ ë‚´ìš©: {hypothetical_doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ìƒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    \n",
    "retrieved_docs = chroma_k_retriever.invoke(hypothetical_doc)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) HyDE ê²€ìƒ‰ê¸° ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_retriever = hyde_chain | chroma_k_retriever\n",
    "\n",
    "retrieved_docs = hyde_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [ì¶œì²˜: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) ìµœì¢… ë‹µë³€ ìƒì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… RAGë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "template = \"\"\"ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n",
    "\n",
    "ì»¨í…ìŠ¤íŠ¸:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "\n",
    "rag_prompt =  ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n",
    "    \n",
    "# RAG ì‹¤í–‰\n",
    "query = \"ë¦¬ë¹„ì•ˆì˜ ì‚¬ì—… ê²½ìŸë ¥ì€ ì–´ë””ì„œ ë‚˜ì˜¤ë‚˜ìš”?\"\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "print(f\"ë‹µë³€: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) HyDE ì²´ì¸ ì¢…í•©`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. ê°€ìƒ ë¬¸ì„œ ìƒì„± ë° ê²€ìƒ‰\n",
    "query = \"í…ŒìŠ¬ë¼ì˜ ê²½ì˜ì§„ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# Step 2. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retrieved_docs = hyde_retriever.invoke({\"question\": query})\n",
    "\n",
    "# Step 3. ìµœì¢… ë‹µë³€ ìƒì„±\n",
    "final_answer = rag_chain.invoke(\n",
    "    {\n",
    "        \"context\": format_docs(retrieved_docs), \n",
    "        \"question\": query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"ì¿¼ë¦¬: {query}\")\n",
    "print(f\"ê°€ìƒ ë¬¸ì„œ ë‚´ìš©: {hypothetical_doc}\")\n",
    "print(f\"ë‹µë³€: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skt-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
